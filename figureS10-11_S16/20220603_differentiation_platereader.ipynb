{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfb70fd6",
   "metadata": {},
   "source": [
    "In the this jupyter notebook, we analyze the plate reader and flow cytometry data associated with Supplementary Figure 10 chacterizing the dose response of integrase induction with salicylate on differentiated cell fraction for 1x and 2x differentiation strains. We also analyze the selective plating data from Supplementary Figure 16 and generate the associated plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d36bf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.optimize\n",
    "import itertools\n",
    "import sys\n",
    "import io\n",
    "import collections\n",
    "import warnings\n",
    "import scipy.interpolate\n",
    "import csv\n",
    "import os\n",
    "import math\n",
    "import seaborn as sns\n",
    "from collections import namedtuple\n",
    "import pkg_resources\n",
    "from datetime import datetime\n",
    "from copy import deepcopy as dc\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from cycler import cycler\n",
    "import numpy as np\n",
    "%matplotlib qt5\n",
    "\n",
    "sns.set_context(\"talk\", font_scale=1.5, rc={\"lines.linewidth\": 1.5})\n",
    "sns.set_style(\"ticks\")\n",
    "sns.set_style({\"xtick.direction\": \"in\",\"ytick.direction\": \"in\"})\n",
    "\n",
    "%config InlineBackend.figure_f.ormats=['svg']\n",
    "\n",
    "mpl.rc('axes', prop_cycle=(cycler('color', ['r', 'k', 'b','g','y','m','c']) ))\n",
    "\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "tw = 1.5\n",
    "sns.set_style({\"xtick.major.size\": 3, \"ytick.major.size\": 3,\n",
    "               \"xtick.minor.size\": 2, \"ytick.minor.size\": 2,\n",
    "               'axes.labelsize': 16, 'axes.titlesize': 16,\n",
    "               'xtick.major.width': tw, 'xtick.minor.width': tw,\n",
    "               'ytick.major.width': tw, 'ytick.minor.width': tw})\n",
    "\n",
    "mpl.rc('xtick', labelsize=14) \n",
    "mpl.rc('ytick', labelsize=14)\n",
    "mpl.rc('axes', linewidth=1.5)\n",
    "mpl.rc('legend', fontsize=14)\n",
    "mpl.rc('figure', figsize=(8.5,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56002645",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Below we define functions for reading csvs of time course plate reader data from a Tecan Infinite 200Pro. Much of this code was adopted from murraylab_tools.biotek to work with Tecan csvs. Can instead read in tidy dfs below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b6261c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ReadSet = collections.namedtuple('ReadSet', ['name','number','mode','excitation','emission','num_flashes','gain'])\n",
    "def mt_open(filename, setting_code):\n",
    "    return_file = io.open(filename, setting_code,\n",
    "                          encoding = \"latin-1\")# encoding)\n",
    "    return return_file\n",
    "def read_supplementary_info(input_filename):\n",
    "    info = dict()\n",
    "    with mt_open(input_filename, 'rU') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        title_line = next(reader)\n",
    "        title_line = list(map(lambda s:s.strip(), title_line))\n",
    "        for i in range(1, len(title_line)):\n",
    "            info[title_line[i]] = dict()\n",
    "        for line in reader:\n",
    "            line = list(map(lambda s:s.strip(), line))\n",
    "            if len(line) == 0 or line[0].strip() == \"\":\n",
    "                continue\n",
    "            for i in range(1, len(title_line)):\n",
    "                info[title_line[i]][line[0]] = line[i]\n",
    "    return info\n",
    "def tidy_tecan_data(input_filename, read_names,supplementary_filename = None):\n",
    "    '''\n",
    "    Convert the raw output from a Biotek plate reader into tidy data.\n",
    "    Optionally, also adds columns of metadata specified by a \"supplementary\n",
    "    file\", which is a CSV spreadsheet mapping well numbers to metadata.\n",
    "    Arguments:\n",
    "        --input_filename: Name of a Tecab output file. Data file should be\n",
    "                            standard excel output files, saved as a CSV.\n",
    "        --supplementary_filename: Name of a supplementary file. Supplementary\n",
    "                                    file must be a CSV wit a header, where the\n",
    "                                    first column is the name of the well,\n",
    "                                    additional columns define additional\n",
    "                                    metadata, and each row after the header is a\n",
    "                                    single well's metadata. Defaults to None\n",
    "                                    (no metadata other than what can be mined\n",
    "                                    from the data file).\n",
    "    Returns: None\n",
    "    Side Effects: Creates a new CSV with the same name as the data file with\n",
    "                    \"_tidy\" appended to the end. This new file is in tidy\n",
    "                    format, with each row representing a single channel read\n",
    "                    from a single well at a single time.\n",
    "    '''\n",
    "\n",
    "    rows = ['A','B','C','D','E','F','G','H']\n",
    "    cols = ['1','2','3','4','5','6','7','8','9','10','11','12']\n",
    "    well_list = [row+col for row in rows for col in cols]\n",
    "    supplementary_data = dict()\n",
    "    if supplementary_filename:\n",
    "        supplementary_data = read_supplementary_info(supplementary_filename)\n",
    "    filename_base   = input_filename.rsplit('.', 1)[0]\n",
    "    output_filename = filename_base + \"_tidy.csv\"\n",
    "\n",
    "#     if calibration_dict is None:\n",
    "#         calibration_dict = calibration_data()\n",
    "\n",
    "    # If the user gave you an excel file, convert it to a CSV so we can read\n",
    "    # it properly.\n",
    "    file_extension = input_filename.rpartition(\".\")[2]\n",
    "    if file_extension.startswith(\"xls\"):\n",
    "        excel_filename = input_filename\n",
    "        input_filename = excel_filename.rpartition(\".\")[0] + \".csv\"\n",
    "        pd.read_excel(excel_filename).to_csv(input_filename, index = False)\n",
    "\n",
    "    # Open data file and tidy output file at once, so that we can stream data\n",
    "    # directly from one to the other without having to store much.\n",
    "   \n",
    "    with mt_open(input_filename, 'rU') as infile:\n",
    "        with mt_open(output_filename, 'w') as outfile:\n",
    "            # Write a header to the tidy output file.\n",
    "            read_num = 0\n",
    "            reader = csv.reader(infile)\n",
    "            writer = csv.writer(outfile, delimiter = ',')\n",
    "            title_row = ['channel','mode','excitation','emission','num_flashes','gain','time_sec','time_h','well','measurement']\n",
    "#             ['channel', 'gain', 'Time (sec)', 'Time (hr)', 'Well',\n",
    "#                          'measurement', 'Units', 'Excitation', 'Emission']\n",
    "            for name in supplementary_data.keys():\n",
    "                title_row.append(name)\n",
    "#             title_row.append('ChanStr')\n",
    "            writer.writerow(title_row)\n",
    "\n",
    "            # Read plate information\n",
    "            # Basic reading flow looks like:\n",
    "            #   1) Read lines until a line that reads \"Read\", recording\n",
    "            #       information about plate reader ID.\n",
    "            #   2) For each line until the next line that reads \"Layout\":\n",
    "            #       2.1) Look for a line starting with \"Filter Set:\"\n",
    "            #       2.2) Get read set information from next two lines, store it.\n",
    "            #   3) Read lines until the line that reads \"Layout\", looking for\n",
    "            #       information about read settings.\n",
    "            #   4) For each line:\n",
    "            #       4.1) If line contains information about this read setting,\n",
    "            #               store it.\n",
    "            #       4.2) If line is the start of data, then for each line until\n",
    "            #             empty line:\n",
    "            #           4.2.1) Rewrite data on that line to tidy data file,\n",
    "            #                   converting to uM if possible.\n",
    "            read_sets = dict()\n",
    "            read_set_idxs = dict()\n",
    "            next_line = \"\"\n",
    "            end_reads = False\n",
    "            while end_reads==False:\n",
    "                if read_num == len(read_names):\n",
    "                    end_reads = True\n",
    "                    continue\n",
    "                if next_line != \"\":\n",
    "                    line = next_line\n",
    "                    next_line = \"\"\n",
    "                else:\n",
    "                    try:\n",
    "                        line = next(reader)\n",
    "                    except StopIteration:\n",
    "                        break\n",
    "                for a in line:\n",
    "                    if len(a):\n",
    "                        break\n",
    "                else:\n",
    "                    continue\n",
    "                if line[0] == \"Mode\":\n",
    "                    mode = line[4]\n",
    "                    read_name = read_names[read_num]\n",
    "                    read_num += 1\n",
    "\n",
    "                    # Process all the information for this read set into one\n",
    "                    # solid text block, which we will search for relevant\n",
    "                    # information\n",
    "                    read_information_block = \"\"\n",
    "                    for line in reader:\n",
    "                        if mode=='Absorbance':\n",
    "                            if line[0] == 'Measurement Wavelength':\n",
    "                                emission = -1\n",
    "                                excitation = line[4]\n",
    "                                gain = -1\n",
    "                                continue\n",
    "                        if line[0]=='Excitation Wavelength':\n",
    "                            excitation = line[4]\n",
    "                            continue\n",
    "                        if line[0]=='Gain':\n",
    "                            gain = line[4]\n",
    "                            continue\n",
    "                        if line[0]=='Emission Wavelength':\n",
    "                            emission = line[4]\n",
    "                            continue\n",
    "                        if line[0]=='Number of Flashes':\n",
    "                            num_flashes=line[4]\n",
    "                        if line[0]=='Settle Time':\n",
    "                            break\n",
    "                        if line[0]=='Start Time:':\n",
    "                            end_reads = True\n",
    "                    if not read_name in read_sets:\n",
    "                        read_sets[read_name] = []\n",
    "                        read_set_idxs[read_name] = 0\n",
    "                    read_sets[read_name].append(ReadSet(read_name,read_num,mode,excitation,\n",
    "                                                    emission, num_flashes,gain))\n",
    "#                     print(read_sets[read_name])\n",
    "                if end_reads:\n",
    "\n",
    "                    break\n",
    "\n",
    "            # Read data blocks\n",
    "            # Find a data block\n",
    "            end_data=False\n",
    "            while end_data==False:\n",
    "                for a in line:\n",
    "                    if len(a):\n",
    "                        break\n",
    "                else:\n",
    "                    line = next(reader, None)\n",
    "                    continue\n",
    "                if line[0] in read_names:\n",
    "                    read_name = line[0]\n",
    "                    read_properties    = read_sets[read_name][0]\n",
    "                    gain = read_properties.gain\n",
    "                    excitation = read_properties.excitation\n",
    "                    emission = read_properties.emission\n",
    "                    num_flashes = read_properties.num_flashes\n",
    "                    read_num = read_properties.number\n",
    "                    mode = read_properties.mode\n",
    "\n",
    "                    line = next(reader) # Skip a line\n",
    "                    line = next(reader) # Chart title line\n",
    "                    times = line[1:]\n",
    "                    line = next(reader)\n",
    "                    # Data lines\n",
    "                    for line in reader:\n",
    "                        if line[0] in well_list:\n",
    "                            well_name = line[0]\n",
    "                        else:\n",
    "                            break\n",
    "                        if supplementary_filename and \\\n",
    "                          not well_name in list(supplementary_data.values())[0]:\n",
    "                            warnings.warn(\"No supplementary data for well \" + \\\n",
    "                                        \"%s; throwing out data for that well.\"\\\n",
    "                                          % well_name)\n",
    "                            continue\n",
    "                        for idx, t in enumerate(times):\n",
    "                            if len(t)==0:\n",
    "                                continue\n",
    "                            t_secs = float(t)\n",
    "                            t_hrs = t_secs/60/60\n",
    "                            measurement = float(line[idx+1])\n",
    "                            row = [read_name, mode, str(excitation),str(emission), num_flashes, gain, \n",
    "                                   t_secs, t_hrs, well_name, measurement]\n",
    "                            for name in supplementary_data.keys():\n",
    "                                row.append(supplementary_data[name][well_name])\n",
    "#                             row.append(read_name + str(gain) + str(excitation) +\\\n",
    "#                                        str(emission))\n",
    "                            try:\n",
    "                                writer.writerow(row)\n",
    "                            except TypeError as e:\n",
    "                                print(\"Error writing line: \" + str(row))\n",
    "                                raise e\n",
    "                        \n",
    "                    if read_num >= len(read_names):\n",
    "                        end_data=True\n",
    "                line = next(reader, None)\n",
    "def background_subtract_df(df, bg_df, by_well=True):\n",
    "    channels = df.channel.unique()\n",
    "    df_bs = df.copy()\n",
    "    for channel in channels:\n",
    "        for gain in df.loc[df.channel==channel,'gain'].unique():\n",
    "            if by_well:\n",
    "                for well in df.well.unique():\n",
    "                    df_bs.loc[(df_bs.channel==channel)&\\\n",
    "                              (df_bs.gain==gain)&\\\n",
    "                              (df_bs.well==well),'measurement'] = \\\n",
    "                    df.loc[(df.channel==channel)&\\\n",
    "                              (df.gain==gain)&\\\n",
    "                              (df.well==well),'measurement'] -\\\n",
    "                    bg_df.loc[(bg_df.channel==channel)&\\\n",
    "                              (bg_df.gain==gain)&\\\n",
    "                              (bg_df.well==well),'measurement'].mean()\n",
    "            else:\n",
    "                df_bs.loc[(df_bs.channel==channel)&\\\n",
    "                              (df_bs.gain==gain),'measurement'] = \\\n",
    "                    df.loc[(df.channel==channel)&\\\n",
    "                              (df.gain==gain),'measurement'] -\\\n",
    "                    bg_df.loc[(bg_df.channel==channel)&\\\n",
    "                              (bg_df.gain==gain),'measurement'].mean()\n",
    "    return df_bs\n",
    "        \n",
    "def normalize(df, norm_channel = \"OD700\", norm_channel_gain = -1):\n",
    "    '''\n",
    "    Normalize expression measurements by dividing each measurement by the value\n",
    "    of a reference channel at that time (default OD600).\n",
    "    Args:\n",
    "        df - DataFrame of time traces, of the kind produced by tidy_biotek_data.\n",
    "        norm_channel - Name of a channel to normalize by. Default \"OD600\"\n",
    "        norm_channel_gain - gain of the channel you want to normalize by.\n",
    "                            Default -1 (for OD600).\n",
    "    Returns:\n",
    "        A DataFrame of df augmented with columns for normalized AFU/uM\n",
    "        (\"Normalized measurement\"). Will have units of\n",
    "        \"<measurement units>/<normalization units>\", or \"<measurement units>/OD\"\n",
    "        if normalizing with an OD.\n",
    "    '''\n",
    "    # Do some kind of check to make sure the norm channel exists with the given\n",
    "    # channel...\n",
    "    if not norm_channel in df.channel.unique():\n",
    "        raise ValueError(\"No data for channel '%s' in dataframe.\" % \\\n",
    "                         norm_channel)\n",
    "    if not norm_channel_gain in df[df.channel == norm_channel].gain.unique():\n",
    "        raise ValueError(\"channel %s does not use gain %d.\" % \\\n",
    "                         (norm_channel, norm_channel_gain))\n",
    "\n",
    "    # Iterate over channels/gains, applying normalization\n",
    "#     OD_channel_string = df[(df.channel == norm_channel) & \\\n",
    "#                  (df.gain== norm_channel_gain)].ChanStr.unique()[0]\n",
    "    od_df = df[df.channel == norm_channel].reset_index()\n",
    "    #dflst = []\n",
    "    channel_list = df.channel.unique().tolist()\n",
    "    del channel_list[channel_list.index(norm_channel)]\n",
    "    dflist = [df[df.channel == a].reset_index() for a in channel_list]\n",
    "    normalized_df = od_df.copy()\n",
    "    for channel_df in dflist:\n",
    "        channel_df.measurement = channel_df.measurement/od_df.measurement\n",
    "        norm_units = \"OD\" if norm_channel.startswith(\"OD\") \\\n",
    "                          else 'afu'\n",
    "#         channel_df.Units = \"%s/%s\" % ('afu', norm_units)\n",
    "        normalized_df = normalized_df.append(channel_df,ignore_index=True)\n",
    "    normalized_df.reset_index()\n",
    "\n",
    "    return normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6329df16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rorywilliams/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: 'U' mode is deprecated\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "tidy_tecan_data('20220604_differentiation_p1.csv',read_names=['OD700','mScarlet','GFP_1'],\n",
    "                supplementary_filename='20220604_metadata.csv')\n",
    "tidy_tecan_data('20220604_differentiation_p1_blankread.csv',read_names=['OD700','mScarlet','GFP_1'],\n",
    "                supplementary_filename='20220604_metadata.csv')\n",
    "tidy_tecan_data('20220604_differentiation_p2.csv',read_names=['OD700','mScarlet','GFP_1'],\n",
    "                supplementary_filename='20220604_metadata.csv')\n",
    "tidy_tecan_data('20220604_differentiation_p2_blankread.csv',read_names=['OD700','mScarlet','GFP_1'],\n",
    "                supplementary_filename='20220604_metadata.csv')\n",
    "tidy_tecan_data('20220604_differentiation_p3.csv',read_names=['OD700','mScarlet','GFP_1'],\n",
    "                supplementary_filename='20220604_metadata.csv')\n",
    "tidy_tecan_data('20220604_differentiation_p3_blankread.csv',read_names=['OD700','mScarlet','GFP_1'],\n",
    "                supplementary_filename='20220604_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa88c90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p1 = pd.read_csv('20220604_differentiation_p1_tidy.csv')\n",
    "df_p1_bg = pd.read_csv('20220604_differentiation_p1_blankread_tidy.csv')\n",
    "df_p2 = pd.read_csv('20220604_differentiation_p2_tidy.csv')\n",
    "df_p2_bg = pd.read_csv('20220604_differentiation_p2_blankread_tidy.csv')\n",
    "df_p3 = pd.read_csv('20220604_differentiation_p3_tidy.csv')\n",
    "df_p3_bg = pd.read_csv('20220604_differentiation_p3_blankread_tidy.csv')\n",
    "\n",
    "# didn't have correct gain of mscarlet for p1, using p2 bg instead\n",
    "df_p1_bg.loc[df_p1_bg.channel=='mScarlet','measurement'] = df_p2_bg.loc[df_p2_bg.channel=='mScarlet','measurement'].mean()\n",
    "df_p1_bg.loc[df_p1_bg.channel=='mScarlet','gain'] =140\n",
    "df_p1_bs = background_subtract_df(df_p1,df_p1_bg,True)\n",
    "df_p2_bs = background_subtract_df(df_p2,df_p2_bg,True)\n",
    "df_p3_bs = background_subtract_df(df_p3,df_p3_bg,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80325dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first timepoint data is dropped because read occured before shaking and is not accurate\n",
    "df_p1_bs['plate'] = 1\n",
    "df_p2_bs['plate'] = 2\n",
    "df_p3_bs['plate'] = 3\n",
    "\n",
    "df_p1_bs = df_p1_bs.drop(df_p1_bs.loc[df_p1_bs.time_h==df_p1_bs.time_h.min()].index.values)\n",
    "df_p2_bs = df_p2_bs.drop(df_p2_bs.loc[df_p2_bs.time_h==df_p2_bs.time_h.min()].index.values)\n",
    "df_p3_bs = df_p3_bs.drop(df_p3_bs.loc[df_p3_bs.time_h==df_p3_bs.time_h.min()].index.values)\n",
    "\n",
    "df_all = pd.concat([df_p1_bs,df_p2_bs,df_p3_bs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d08b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv('figureS10_platereader_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8f90529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OD normalize data, get endpoint data, avgs and stds\n",
    "df_p1_bs_norm = normalize(df_p1_bs)\n",
    "df_p1_bs_norm_GFP = df_p1_bs_norm.loc[df_p1_bs_norm.channel=='GFP_1',:]\n",
    "df_p1_ep_GFP = df_p1_bs_norm_GFP.loc[df_p1_bs_norm_GFP.time_h==df_p1_bs_norm_GFP.time_h.max(),:]\n",
    "df_p1_OD = df_p1_bs_norm.loc[df_p1_bs_norm.channel=='OD700',:]\n",
    "df_p1_OD_ep = df_p1_OD.loc[df_p1_OD.time_h==df_p1_OD.time_h.max(),:]\n",
    "\n",
    "df_p2_bs_norm = normalize(df_p2_bs)\n",
    "df_p2_bs_norm_GFP = df_p2_bs_norm.loc[df_p2_bs_norm.channel=='GFP_1',:]\n",
    "df_p2_ep_GFP = df_p2_bs_norm_GFP.loc[df_p2_bs_norm_GFP.time_h==df_p2_bs_norm_GFP.time_h.max(),:]\n",
    "df_p2_OD = df_p2_bs_norm.loc[df_p2_bs_norm.channel=='OD700',:]\n",
    "df_p2_OD_ep = df_p2_OD.loc[df_p2_OD.time_h==df_p2_OD.time_h.max(),:]\n",
    "\n",
    "df_p3_bs_norm = normalize(df_p3_bs)\n",
    "df_p3_bs_norm_GFP = df_p3_bs_norm.loc[df_p3_bs_norm.channel=='GFP_1',:]\n",
    "df_p3_ep_GFP = df_p3_bs_norm_GFP.loc[df_p3_bs_norm_GFP.time_h==df_p3_bs_norm_GFP.time_h.max(),:]\n",
    "df_p3_OD = df_p3_bs_norm.loc[df_p3_bs_norm.channel=='OD700',:]\n",
    "df_p3_OD_ep = df_p3_OD.loc[df_p3_OD.time_h==df_p3_OD.time_h.max(),:]\n",
    "\n",
    "df_bs_norm = pd.concat([df_p1_bs_norm,df_p2_bs_norm,df_p3_bs_norm])\n",
    "df_gfp_norm_ep = pd.concat([df_p1_ep_GFP,df_p2_ep_GFP,df_p3_ep_GFP])\n",
    "df_OD_ep = pd.concat([df_p1_OD_ep,df_p2_OD_ep,df_p3_OD_ep])\n",
    "\n",
    "df_OD_ep_avg = df_OD_ep.groupby(['strain','iptg','sal','chlor','plate'],as_index=False)['measurement'].mean()\n",
    "df_OD_ep_avg['std'] = df_OD_ep.groupby(['strain','iptg','sal','chlor','plate'])['measurement'].std().values\n",
    "df_gfp_norm_ep_avg = df_gfp_norm_ep.groupby(['strain','iptg','sal','chlor','plate'],as_index=False)['measurement'].mean()\n",
    "df_gfp_norm_ep_avg['std'] = df_gfp_norm_ep.groupby(['strain','iptg','sal','chlor','plate'])['measurement'].std().values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a44c9c",
   "metadata": {},
   "source": [
    "Below we adopt code from the murray_lab_tools.biotek package (courtesy of Sam Clamons) for fitting growth rates on all wells. We make the modification of using an exponential growth function, and trim data strictly by OD value before fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29495223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_growth(channel_df, fixed_init = None, growth_threshold = None,\n",
    "                     verbose = False, growth='logistic'):\n",
    "    '''\n",
    "    Summarizes the growth characteristics of OD curves from a dataframe of\n",
    "    Biotek/platereader data. Performs the following summaries on each well:\n",
    "        * Fits OD curve to a logistic-plus-floor or exponential-plus-floor, finding an initial value, a\n",
    "            noise floor, a rate constant (R, not to be interpreted directly),\n",
    "            and a population maximum. The rate parameter has time units of\n",
    "            hours.\n",
    "        * Optionally finds the time when the population crosses some fraction of\n",
    "            maximum population. By default, does not calculate this -- set\n",
    "            the growth_threshold parameter to add this calculation.\n",
    "    Params:\n",
    "        df -- A DataFrame of Biotek data with at least one channel of growth\n",
    "                data.\n",
    "        channel -- The name of the channel with growth data. Should be a channel\n",
    "                    with only one Gain, or this will do weird things.\n",
    "        growth_threshold -- If set, determines the fraction of of total\n",
    "                                population to find the time of, i.e., if\n",
    "                                growth_threshold = 0.25, this function will\n",
    "                                report the time that each well crossed 25%\n",
    "                                of the total population size for that well.\n",
    "        fixed_init -- Sets a fixed value for the initial population parameter.\n",
    "                        If None, this value is optimized with the rest of the\n",
    "                        parameters.\n",
    "        verbose -- Iff True, prints some hints about what it's doing. Use if it's taking\n",
    "                    a while and you want to make sure it's making progress. Default False.\n",
    "    Returns: A new dataframe where each row summarizes the growth\n",
    "                characteristics of one from the original dataframe.\n",
    "    '''\n",
    "\n",
    "    # Split dataframe into a list of dataframes for individual wells.\n",
    "    well_dfs = [channel_df[channel_df.well == w] \\\n",
    "                for w in channel_df.well.unique()]\n",
    "    measurement_summary_rows = \\\n",
    "        list(map(lambda df: summarize_single_well_growth(df, growth_threshold,\n",
    "                                                         fixed_init, verbose,growth),\n",
    "                 well_dfs))\n",
    "    return pd.DataFrame(measurement_summary_rows)\n",
    "\n",
    "def summarize_single_well_growth(well_df, growth_threshold = None,\n",
    "                                 fixed_init = None, verbose = False,growth='logistic'):\n",
    "    '''\n",
    "    Summarizes the growth characteristics of a single well's worth of dataframe,\n",
    "    returning the results as a dictionary describing a single dataframe line.\n",
    "    See summarize_growth for measurement details.\n",
    "    This function is intended as a helper function for summarize_growth; it uses\n",
    "    the helper function logistic_growth as a growth model.\n",
    "    Params:\n",
    "        well_df -- A DataFrame of Biotek data from a single well and a single\n",
    "                channel (presumably an OD channel).\n",
    "        growth_threshold -- If set, determines the fraction of of total\n",
    "                                population to find the time of, i.e., if\n",
    "                                growth_threshold = 0.25, this function will\n",
    "                                report the time that each well crossed 25%\n",
    "                                of the total population size for that well.\n",
    "        fixed_init -- Sets a fixed value for the initial population parameter.\n",
    "                        If None, this value is optimized with the rest of the\n",
    "                        parameters.\n",
    "        verbose -- Iff True, print some hints about how it's progressing.\n",
    "    Returns: A dictionary containing the well name, growth characteristics, and\n",
    "                any supplementary data from df.\n",
    "    '''\n",
    "    well_df.reset_index(inplace = True)\n",
    "    if growth_threshold is not None:\n",
    "        well_df = well_df.loc[well_df.measurement<growth_threshold,:]\n",
    "    if verbose:\n",
    "        print(\"Summarizing from well %s\" % well_df.well[0])\n",
    "\n",
    "    # Some empirically-reasonable guesses for most growth experiments.\n",
    "    if growth == 'logistic':\n",
    "        if fixed_init == None:\n",
    "            param_guess = (1.3, 1, 0.05, 0)\n",
    "            opt_func = logistic_growth\n",
    "        else:\n",
    "            param_guess = (1.3, 1, 0.05)\n",
    "            opt_func = lambda t, r, c, f: logistic_growth(t, r, c, f, fixed_init)\n",
    "\n",
    "        times = well_df[\"time_h\"]\n",
    "\n",
    "        opt_params = scipy.optimize.curve_fit(opt_func, times,\n",
    "                                              well_df[\"measurement\"],\n",
    "                                              p0 = param_guess,\n",
    "                                              maxfev = int(1e4))[0]\n",
    "\n",
    "        # To keep parameters positive, logistic_growth uses the absolute value\n",
    "        # of whatever parameters it gets, so optimization will sometimes return\n",
    "        # negative parameter values; have to correct these.\n",
    "        opt_params = np.abs(opt_params)\n",
    "\n",
    "        return_dict = dict()\n",
    "        return_dict[\"Rate\"]  = opt_params[0]\n",
    "        return_dict[\"Cap\"]   = opt_params[1]\n",
    "        return_dict[\"Floor\"] = opt_params[2]\n",
    "        if fixed_init == None:\n",
    "            return_dict[\"Init\"] = opt_params[3]\n",
    "        else:\n",
    "            return_dict[\"Init\"] = fixed_init\n",
    "\n",
    "        # Calculate threshold time, if it is specified\n",
    "    #     if growth_threshold:\n",
    "    #         raise NotImplementedError()\n",
    "\n",
    "        # Add supplemental data.\n",
    "        for column in well_df.columns.values:\n",
    "            if column in ['channel', 'mode', 'excitation', 'emission', 'num_flashes', 'gain',\n",
    "       'time_sec', 'time_h', 'measurement']:\n",
    "                continue\n",
    "            return_dict[column] = well_df[column][0]\n",
    "    elif growth == 'exponential':\n",
    "        if fixed_init == None:\n",
    "            param_guess = (1.3, 0.001, 0)\n",
    "            opt_func = exp_growth\n",
    "        else:\n",
    "            param_guess = (1.3, 0.001)\n",
    "            opt_func = lambda t, r, f: exp_growth(t, r, f, fixed_init)\n",
    "\n",
    "        times = well_df[\"time_h\"]\n",
    "\n",
    "        opt_params = scipy.optimize.curve_fit(opt_func, times,\n",
    "                                              well_df[\"measurement\"],\n",
    "                                              p0 = param_guess,\n",
    "                                              maxfev = int(1e4))[0]\n",
    "\n",
    "        # To keep parameters positive, logistic_growth uses the absolute value\n",
    "        # of whatever parameters it gets, so optimization will sometimes return\n",
    "        # negative parameter values; have to correct these.\n",
    "        opt_params = np.abs(opt_params)\n",
    "\n",
    "        return_dict = dict()\n",
    "        return_dict[\"Rate\"]  = opt_params[0]\n",
    "        return_dict[\"Floor\"] = opt_params[1]\n",
    "        if fixed_init == None:\n",
    "            return_dict[\"Init\"] = opt_params[2]\n",
    "        else:\n",
    "            return_dict[\"Init\"] = fixed_init\n",
    "\n",
    "        # Calculate threshold time, if it is specified\n",
    "    #     if growth_threshold:\n",
    "    #         raise NotImplementedError()\n",
    "\n",
    "        # Add supplemental data.\n",
    "        for column in well_df.columns.values:\n",
    "            if column in ['channel', 'mode', 'excitation', 'emission', 'num_flashes', 'gain',\n",
    "       'time_sec', 'time_h', 'measurement']:\n",
    "                continue\n",
    "            return_dict[column] = well_df[column][0]\n",
    "\n",
    "    return return_dict\n",
    "\n",
    "def logistic_growth(t, rate, cap, floor, init):\n",
    "    '''\n",
    "    Model function for logistic growth with a noise floor.\n",
    "    Params:\n",
    "        t -- Time.\n",
    "        rate -- Growth rate parameter.\n",
    "        init -- Initial population.\n",
    "        cap -- Maximum population size.\n",
    "        floor -- Noise floor (i.e., OD reading for zero cells).\n",
    "    Returns: Model OD reading at the given time, for the given parameters.\n",
    "    '''\n",
    "\n",
    "    rate = np.abs(rate)\n",
    "    init = np.abs(init)\n",
    "    cap = np.abs(cap)\n",
    "    floor = np.abs(floor)\n",
    "    return floor + cap * init * np.exp(rate * t) \\\n",
    "            / (cap + init * (np.exp(rate * t) - 1))\n",
    "\n",
    "def exp_growth(t, rate, floor, init):\n",
    "    '''\n",
    "    Model function for logistic growth with a noise floor.\n",
    "    Params:\n",
    "        t -- Time.\n",
    "        rate -- Growth rate parameter.\n",
    "        init -- Initial population.\n",
    "        floor -- Noise floor (i.e., OD reading for zero cells).\n",
    "    Returns: Model OD reading at the given time, for the given parameters.\n",
    "    '''\n",
    "\n",
    "    rate = np.abs(rate)\n",
    "    init = np.abs(init)\n",
    "    floor = np.abs(floor)\n",
    "    return floor + init * np.exp(rate * t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b95143b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine growth rates using OD700<0.2 to fit exponential growth function\n",
    "p1_growth_df = summarize_growth(df_p1_bs.loc[df_p1_bs.channel=='OD700',:],\n",
    "                                  growth='exponential',growth_threshold=0.2)\n",
    "p1_growth_df_avg = p1_growth_df.groupby(['strain','iptg','sal','chlor'],as_index=False)['Rate'].mean()\n",
    "p1_growth_df_avg['std'] = p1_growth_df.groupby(['strain','iptg','sal','chlor'])['Rate'].std().values\n",
    "p2_growth_df = summarize_growth(df_p2_bs.loc[df_p2_bs.channel=='OD700',:],\n",
    "                                  growth='exponential',growth_threshold=0.2)\n",
    "p2_growth_df_avg = p2_growth_df.groupby(['strain','iptg','sal','chlor'],as_index=False)['Rate'].mean()\n",
    "p2_growth_df_avg['std'] = p2_growth_df.groupby(['strain','iptg','sal','chlor'])['Rate'].std().values\n",
    "p3_growth_df = summarize_growth(df_p3_bs.loc[df_p3_bs.channel=='OD700',:],\n",
    "                                  growth='exponential',growth_threshold=0.2)\n",
    "p3_growth_df_avg = p3_growth_df.groupby(['strain','iptg','sal','chlor'],as_index=False)['Rate'].mean()\n",
    "p3_growth_df_avg['std'] = p3_growth_df.groupby(['strain','iptg','sal','chlor'])['Rate'].std().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3589eb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_growth_df['plate'] = 1\n",
    "p2_growth_df['plate'] = 2\n",
    "p3_growth_df['plate'] = 3\n",
    "p1_growth_df_avg['plate'] = 1\n",
    "p2_growth_df_avg['plate'] = 2\n",
    "p3_growth_df_avg['plate'] = 3\n",
    "growth_df = pd.concat([p1_growth_df,p2_growth_df,p3_growth_df])\n",
    "growth_df_avg = pd.concat([p1_growth_df_avg,p2_growth_df_avg,p3_growth_df_avg])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116954bd",
   "metadata": {},
   "source": [
    "Now we will read in the flow cytometry data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9360129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('20220604_metadata.csv')\n",
    "p1 = pd.read_csv('./flow_data/plate1_flow.csv')\n",
    "p2 = pd.read_csv('./flow_data/plate2_flow.csv')\n",
    "p3 = pd.read_csv('./flow_data/plate3_flow.csv')\n",
    "metadata.drop(columns=['well'],inplace=True)\n",
    "p1 = pd.concat([metadata,p1],axis=1)\n",
    "p2 = pd.concat([metadata,p2],axis=1)\n",
    "p3 = pd.concat([metadata,p3],axis=1)\n",
    "df_flow = pd.concat([p1,p2,p3],axis=0)\n",
    "df_flow['GFP+_percent'] = df_flow['mScarlet+/GFP+_percent'] + df_flow['mScarlet-/GFP+_percent']\n",
    "df_flow_gfp_avg = df_flow.groupby(['strain','iptg','sal','chlor','Plate'],as_index=False)['GFP+_percent'].mean()\n",
    "df_flow_gfp_avg['std'] =  df_flow.groupby(['strain','iptg','sal','chlor','Plate'])['GFP+_percent'].std().values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c692918",
   "metadata": {},
   "source": [
    "Below we generate the plots as shown in Supplementary Figure 10B-E. Blue is 10uM IPTG, orange is 50uM IPTG and plate generates 1-3 are plotted side by side and are light to dark. Mean +/- SD plotted for all, with individual replicates as small circles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7194ca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4,4,sharex=True, sharey='row',figsize=(8.5,10))\n",
    "\n",
    "iptgs = [10,50]\n",
    "sals = [0,10,20,30]\n",
    "chlors = ['-','+']\n",
    "iptg_offsets = np.linspace(-1,1,6)*2.5\n",
    "colors = ['#6baed6','#2171b5','#08306b','#fd8d3c','#d94801','#7f2704']\n",
    "strains = ['diff1X','diff2X']\n",
    "\n",
    "df = df_gfp_norm_ep\n",
    "df_avg = df_gfp_norm_ep_avg\n",
    "gfp_max = df_gfp_norm_ep_avg.measurement.max()\n",
    "for i, chlor in enumerate(chlors):\n",
    "    for j, strain in enumerate(strains):\n",
    "        plot_num = 0\n",
    "        for k, iptg in enumerate(iptgs):\n",
    "            for p, plate in enumerate([1,2,3]):\n",
    "                ax[0,j*2+i].errorbar(df_avg.loc[(df_avg.strain==strain)&\\\n",
    "                                       (df_avg.chlor==chlor)&\\\n",
    "                                       (df_avg.iptg==iptg)&\\\n",
    "                                       (df_avg.plate==plate),'sal']+iptg_offsets[plot_num],\n",
    "                            df_avg.loc[(df_avg.strain==strain)&\\\n",
    "                                       (df_avg.chlor==chlor)&\\\n",
    "                                       (df_avg.iptg==iptg)&\\\n",
    "                                       (df_avg.plate==plate),'measurement']/gfp_max,\n",
    "                   yerr=df_avg.loc[(df_avg.strain==strain)&\\\n",
    "                                       (df_avg.chlor==chlor)&\\\n",
    "                                       (df_avg.iptg==iptg)&\\\n",
    "                                       (df_avg.plate==plate),'std']/gfp_max,\n",
    "                   capsize=1.5,color=colors[plot_num],linestyle='None',marker='o',ms=3)\n",
    "                plot_num+=1\n",
    "        ax[0,j*2+i].set_title(f'{strain}:{chlor}chlor',fontsize=12)\n",
    "# ax[0,0].legend(['1O-P1','10-P2','10-P3','5O-P1','50-P2','50-P3'],title='IPTG',fontsize=12,title_fontsize=12)\n",
    "for i, chlor in enumerate(chlors):\n",
    "    for j, strain in enumerate(strains):\n",
    "        plot_num = 0\n",
    "        for k, iptg in enumerate(iptgs):\n",
    "            for p, plate in enumerate([1,2,3]):\n",
    "                ax[0,j*2+i].plot(df.loc[(df.strain==strain)&\\\n",
    "                                    (df.chlor==chlor)&\\\n",
    "                                    (df.iptg==iptg)&\\\n",
    "                                    (df.plate==plate),'sal']+iptg_offsets[plot_num],\n",
    "                             df.loc[(df.strain==strain)&\\\n",
    "                                    (df.chlor==chlor)&\\\n",
    "                                    (df.iptg==iptg)&\\\n",
    "                                    (df.plate==plate),'measurement']/gfp_max,\n",
    "                '.',color=colors[plot_num],linestyle='None',ms=2,alpha=0.5)\n",
    "\n",
    "                plot_num+=1\n",
    "ax[0,0].set_ylabel('GFP/OD (a.u.)',fontsize=12)      \n",
    "\n",
    "df = df_flow\n",
    "df_avg = df_flow_gfp_avg\n",
    "for i, chlor in enumerate(chlors):\n",
    "    for j, strain in enumerate(strains):\n",
    "        plot_num = 0\n",
    "        for k, iptg in enumerate(iptgs):\n",
    "            for p, plate in enumerate([1,2,3]):\n",
    "                ax[1,j*2+i].errorbar(df_avg.loc[(df_avg.strain==strain)&\\\n",
    "                                       (df_avg.chlor==chlor)&\\\n",
    "                                       (df_avg.iptg==iptg)&\\\n",
    "                                       (df_avg.Plate==plate),'sal']+iptg_offsets[plot_num],\n",
    "                            df_avg.loc[(df_avg.strain==strain)&\\\n",
    "                                       (df_avg.chlor==chlor)&\\\n",
    "                                       (df_avg.iptg==iptg)&\\\n",
    "                                       (df_avg.Plate==plate),'GFP+_percent'],\n",
    "                   yerr=df_avg.loc[(df_avg.strain==strain)&\\\n",
    "                                       (df_avg.chlor==chlor)&\\\n",
    "                                       (df_avg.iptg==iptg)&\\\n",
    "                                       (df_avg.Plate==plate),'std'],\n",
    "                   capsize=1.5,color=colors[plot_num],linestyle='None',marker='o',ms=3)\n",
    "                plot_num+=1\n",
    "for i, chlor in enumerate(chlors):\n",
    "    for j, strain in enumerate(strains):\n",
    "        plot_num = 0\n",
    "        for k, iptg in enumerate(iptgs):\n",
    "            for p, plate in enumerate([1,2,3]):\n",
    "                ax[1,j*2+i].plot(df.loc[(df.strain==strain)&\\\n",
    "                                    (df.chlor==chlor)&\\\n",
    "                                    (df.iptg==iptg)&\\\n",
    "                                    (df.Plate==plate),'sal']+iptg_offsets[plot_num],\n",
    "                             df.loc[(df.strain==strain)&\\\n",
    "                                    (df.chlor==chlor)&\\\n",
    "                                    (df.iptg==iptg)&\\\n",
    "                                    (df.Plate==plate),'GFP+_percent'],\n",
    "                '.',color=colors[plot_num],linestyle='None',ms=2,alpha=0.5)\n",
    "\n",
    "                plot_num+=1\n",
    "ax[1,0].set_ylabel('GFP+ percent',fontsize=12) \n",
    "\n",
    "df = df_OD_ep\n",
    "df_avg = df_OD_ep_avg               \n",
    "for i, chlor in enumerate(chlors):\n",
    "    for j, strain in enumerate(strains):\n",
    "        plot_num = 0\n",
    "        for k, iptg in enumerate(iptgs):\n",
    "            for p, plate in enumerate([1,2,3]):\n",
    "                ax[2,j*2+i].errorbar(df_avg.loc[(df_avg.strain==strain)&\\\n",
    "                                       (df_avg.chlor==chlor)&\\\n",
    "                                       (df_avg.iptg==iptg)&\\\n",
    "                                       (df_avg.plate==plate),'sal']+iptg_offsets[plot_num],\n",
    "                            df_avg.loc[(df_avg.strain==strain)&\\\n",
    "                                       (df_avg.chlor==chlor)&\\\n",
    "                                       (df_avg.iptg==iptg)&\\\n",
    "                                       (df_avg.plate==plate),'measurement'],\n",
    "                   yerr=df_avg.loc[(df_avg.strain==strain)&\\\n",
    "                                       (df_avg.chlor==chlor)&\\\n",
    "                                       (df_avg.iptg==iptg)&\\\n",
    "                                       (df_avg.plate==plate),'std'],\n",
    "                   capsize=1.5,color=colors[plot_num],linestyle='None',marker='o',ms=3)\n",
    "                plot_num+=1\n",
    "#         ax[j*2+i,0].legend(['1O-P1','10-P2','10-P3''5O-P1','50-P2','50-P3'],title='IPTG',fontsize=12,title_fontsize=12)\n",
    "#         ax[2,j*2+i].set_title(f'{strain}:{chlor}chlor',fontsize=12)\n",
    "for i, chlor in enumerate(chlors):\n",
    "    for j, strain in enumerate(strains):\n",
    "        plot_num = 0\n",
    "        for k, iptg in enumerate(iptgs):\n",
    "            for p, plate in enumerate([1,2,3]):\n",
    "                ax[2,j*2+i].plot(df.loc[(df.strain==strain)&\\\n",
    "                                    (df.chlor==chlor)&\\\n",
    "                                    (df.iptg==iptg)&\\\n",
    "                                    (df.plate==plate),'sal']+iptg_offsets[plot_num],\n",
    "                             df.loc[(df.strain==strain)&\\\n",
    "                                    (df.chlor==chlor)&\\\n",
    "                                    (df.iptg==iptg)&\\\n",
    "                                    (df.plate==plate),'measurement'],\n",
    "                '.',color=colors[plot_num],linestyle='None',ms=2,alpha=0.5)\n",
    "\n",
    "                plot_num+=1\n",
    "ax[2,0].set_ylabel('OD700',fontsize=12) \n",
    "ax[2,0].set_yticks([0,0.5,1])\n",
    "\n",
    "df = growth_df\n",
    "df_avg = growth_df_avg                \n",
    "for i, chlor in enumerate(chlors):\n",
    "    for j, strain in enumerate(strains):\n",
    "        plot_num = 0\n",
    "        for k, iptg in enumerate(iptgs):\n",
    "            for p, plate in enumerate([1,2,3]):\n",
    "                ax[3,j*2+i].errorbar(df_avg.loc[(df_avg.strain==strain)&\\\n",
    "                                       (df_avg.chlor==chlor)&\\\n",
    "                                       (df_avg.iptg==iptg)&\\\n",
    "                                       (df_avg.plate==plate),'sal']+iptg_offsets[plot_num],\n",
    "                            df_avg.loc[(df_avg.strain==strain)&\\\n",
    "                                       (df_avg.chlor==chlor)&\\\n",
    "                                       (df_avg.iptg==iptg)&\\\n",
    "                                       (df_avg.plate==plate),'Rate'],\n",
    "                   yerr=df_avg.loc[(df_avg.strain==strain)&\\\n",
    "                                       (df_avg.chlor==chlor)&\\\n",
    "                                       (df_avg.iptg==iptg)&\\\n",
    "                                       (df_avg.plate==plate),'std'],\n",
    "                   capsize=1.5,color=colors[plot_num],linestyle='None',marker='o',ms=3)\n",
    "                plot_num+=1\n",
    "for i, chlor in enumerate(chlors):\n",
    "    for j, strain in enumerate(strains):\n",
    "        plot_num = 0\n",
    "        for k, iptg in enumerate(iptgs):\n",
    "            for p, plate in enumerate([1,2,3]):\n",
    "                ax[3,j*2+i].plot(df.loc[(df.strain==strain)&\\\n",
    "                                    (df.chlor==chlor)&\\\n",
    "                                    (df.iptg==iptg)&\\\n",
    "                                    (df.plate==plate),'sal']+iptg_offsets[plot_num],\n",
    "                             df.loc[(df.strain==strain)&\\\n",
    "                                    (df.chlor==chlor)&\\\n",
    "                                    (df.iptg==iptg)&\\\n",
    "                                    (df.plate==plate),'Rate'],\n",
    "                '.',color=colors[plot_num],linestyle='None',ms=2,alpha=0.5)\n",
    "\n",
    "                plot_num+=1\n",
    "ax[3,0].set_ylabel('growthrate (1/h)',fontsize=12)   \n",
    "ax[3,0].set_xticks([0,10,20,30])\n",
    "ax[3,0].set_xlabel(f'Sal conc. ($\\mu$M)',fontsize=12)\n",
    "ax[3,1].set_xlabel(f'Sal conc. ($\\mu$M)',fontsize=12)\n",
    "ax[3,2].set_xlabel(f'Sal conc. ($\\mu$M)',fontsize=12)\n",
    "ax[3,3].set_xlabel(f'Sal conc. ($\\mu$M)',fontsize=12)\n",
    "# plt.savefig('20220728_epGFP_fracGFP_epOD_growthrate_wlegend.pdf',transparent=True)\n",
    "plt.savefig('20220728_epGFP_fracGFP_epOD_growthrate.pdf',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45d86df",
   "metadata": {},
   "source": [
    "Below we analyze data from selective plating experiment performed on glycerol stocks from the endpoint of the experiment described in Figure 2. We load in a summary csv and generate the plot shown in Supplementary Figure 16B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e05c743a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plating_df = pd.read_csv('colony_plating_summmary.csv')\n",
    "plating_df_tidy = pd.melt(plating_df,\n",
    "                       ['Replicate','Strain'],\n",
    "                       var_name=\"population\",\n",
    "                       value_name=\"proportion\")\n",
    "plating_df_tidy.dropna(inplace=True)\n",
    "sns.set_palette('colorblind',4)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.barplot(data=plating_df_tidy,x='Strain',y='proportion',hue='population',dodge=True,ax=ax)\n",
    "sns.stripplot(data=plating_df_tidy,x='Strain',y='proportion',hue='population',dodge=True,ax=ax,color='black')\n",
    "plt.savefig('20220809_plating_plot.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702d3c96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
